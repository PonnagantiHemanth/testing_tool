#!/usr/bin/env python
# -*- coding: utf-8 -*-
# ------------------------------------------------------------------------------
# Python Test Harness
# ------------------------------------------------------------------------------
""" @package pylibrary.tools.rttprofiler

@brief  RTT Profiler tools

@author christophe.roquebert
@author nestor.lopez

@date   2019/11/27
"""
# ------------------------------------------------------------------------------
# imports
# ------------------------------------------------------------------------------
import sys
import time
import threading
import _thread
import collections
from typing import Tuple
from typing import Optional
from typing import Union
from dataclasses import dataclass
from statistics import mean
from pylink.jlink import JLink
import pylink.errors
import warnings

# ------------------------------------------------------------------------------
# Constants
# ------------------------------------------------------------------------------
TAG_OFFSET = 0
TAG_SIZE = 2
CYCLES_OFFSET = 2
CYCLES_SIZE = 4
SEQ_NUMBER_OFFSET = 6
SEQ_NUMBER_SIZE = 2

MARKER_SIZE = 2
PAYLOAD_SIZE = TAG_SIZE + SEQ_NUMBER_SIZE + CYCLES_SIZE
HEADER = 0x1010
FOOTER = 0x2020
RECORD_SIZE = 2 * MARKER_SIZE + PAYLOAD_SIZE

Verbose = False
# ------------------------------------------------------------------------------
# implementation
# ------------------------------------------------------------------------------


@dataclass(frozen=True, eq=True)
class ProfilerRecord:
    tag: int
    seq_number : int
    cycles : int
# end class ProfilerRecord


@dataclass(frozen=True, eq=True)
class RelativeMeasure:
    start: int
    end: int
#end class RelativeMeasure

@dataclass(frozen=True, eq=True)
class Statistics:
    min: float
    max: float
    average : float

def counts_to_msec(counts : int) -> float:
    return counts / 64_000
# end def counts_to_msec


class Profiler:
    def __init__(self, startup_tag: int, startup_gap_idx: int) -> None:
        """Constructor method

        :param startup_tag: Tag generated by the firmware at the end of the startup
        :type startup_tag: ``int``
        :param startup_gap_idx: Maximum number of chunk in a RTT buffer
        :type startup_gap_idx: ``int``
        """
        self._in_records = []  # type : List[ProfilerRecord]
        self._startup_tag = startup_tag  # type : int
        self._startup_gap_idx = startup_gap_idx  # type : int
        self._tags_index = collections.defaultdict(dict)  # type : Defaultdict[Dict]
        self.missing_records = 0

    def add_data(self, profiler_records : Tuple[ProfilerRecord, ...]) -> None:
        """ Add a stream of data

        :param profiler_records: Parsed data retrieved by the ProfilerExecutor run function
        :type profiler_records: ``Tuple[ProfilerRecord, ...]``
        """
        self._in_records.extend(profiler_records)

    def clear_data(self):
        """
        Clear the Profiler internal variables
        """
        self._in_records = []
        self._tags_index = collections.defaultdict(dict)
    # end def clear_data

    def _process_startup_sequence(self):
        startup_tag_count = 0
        startup_tag_idx = 0
        # finds if there is a startup tag in the captured stream
        for idx, record in enumerate(self._in_records):
            if record.tag == self._startup_tag:
                startup_tag_idx = idx
                startup_tag_count += 1
            # end if
        assert startup_tag_count == 0 or startup_tag_count == 1, f'startup_tag_count={startup_tag_count}'

        # if there are missing sequence numbers after the first buffer capture, then
        # discard the data before the sequence gap, (but keep the startup sequence if present)
        if (len(self._in_records) > self._startup_gap_idx and
            self._in_records[self._startup_gap_idx - 1].seq_number + 1 != self._in_records[self._startup_gap_idx].seq_number):
            if startup_tag_count > 0:
                del self._in_records[startup_tag_idx + 1:self._startup_gap_idx]
            else:
                del self._in_records[:self._startup_gap_idx]
            # end if
        # end if
    # end def _process_startup_sequence

    def process_data(self):
        self._process_startup_sequence()
        # This builds a "list" of dictionaries where each dict's keys are
        # the tags in the records and the value is a list of indexes of the
        # records where the tag appears.
        # There is one dict for each contiguous section of seq_numbers, so
        # in the case that there are no missing seq_counters, the list of
        # dicts only contains one dict. The "list" of dicts is actually
        # implemented as a dict itself, so as to be able to use defaultdict()
        tags_index_table_idx = 0
        if len(self._in_records) > 0:
            seq_number = self._in_records[0].seq_number
            self._tags_index[0] = collections.defaultdict(list)
            for idx, record in enumerate(self._in_records):
                if record.seq_number != seq_number:
                    # If the previous record matches the startup tag, do not raise the warning
                    if self._in_records[idx-1].tag != self._startup_tag:
                        self.missing_records += (record.seq_number - seq_number)
                    # end if
                    tags_index_table_idx += 1
                    seq_number = (record.seq_number + 1) if record.seq_number < 0xFFFF else 0
                    self._tags_index[tags_index_table_idx] = collections.defaultdict(list)
                else:
                    seq_number = (seq_number + 1) if seq_number < 0xFFFF else 0
                # end if
                self._tags_index[tags_index_table_idx][record.tag].append(idx)
            # end for
        # end if
        if self.missing_records > 0:
            warnings.warn(f'missing records = {self.missing_records}')
        # end if
    # end def process_data

    def get_statistics(self, relative_measurements: Tuple[RelativeMeasure, ...], start=None, end=None) -> dict:
        relative = {key: [] for key in relative_measurements}
        mins = dict.fromkeys(relative_measurements, sys.maxsize)
        maxs = dict.fromkeys(relative_measurements, 0)
        average = dict.fromkeys(relative_measurements, 0)

        for rel in relative.keys():
            for tags_dicts in self._tags_index.values():
                for idx_end in tags_dicts[rel.end]:
                    for idx_start in reversed(tags_dicts[rel.start]):
                        if idx_start < idx_end:
                            relative[rel].append((idx_start, idx_end))
                            break
                        # end if
                    # end for
                # end for
            # end for
        # end for

        for rel in relative.keys():
            if len(relative[rel]) == 0:
                break
            # end if
            all_timestamps = [(self._in_records[start].cycles,
                               self._in_records[end].cycles) for start, end in relative[rel]]
            filtered_timestamps = []
            # Select timings between the given start time and end time
            for start_ts, end_ts in all_timestamps:
                if end is not None and counts_to_msec(start_ts) > end * 1000:
                    break
                elif start is not None and counts_to_msec(end_ts) < start * 1000:
                    continue
                # end if
                filtered_timestamps.append((start_ts, end_ts))
            # end for
            mins[rel] = min((end_ts - start_ts) & 0xFFFFFFFF for start_ts, end_ts in filtered_timestamps)
            maxs[rel] = max((end_ts - start_ts) & 0xFFFFFFFF for start_ts, end_ts in filtered_timestamps)
            average[rel] = mean((end_ts - start_ts) & 0xFFFFFFFF for start_ts, end_ts in filtered_timestamps)
        # end for
        mins = {key: counts_to_msec(mins[key]) if mins[key] < sys.maxsize else 0.0 for key in mins}
        maxs = {key: counts_to_msec(maxs[key]) for key in maxs}
        average = {key: counts_to_msec(average[key]) for key in average}

        statistics = {key : Statistics(min=mins[key], max=maxs[key], average=average[key]) for key in relative.keys()}

        if Verbose:
            for rel in relative.keys():
                print("tag pair:%s, min:%f, max:%f, ave:%f" % (rel,
                   mins[rel],
                   maxs[rel],
                   average[rel]))
            # end for
        # end if
        return statistics
    # end def get_min_max

    def get_startup_time_msec(self) -> Union[None, float]:
        for tags_dicts in self._tags_index.values():
            if self._startup_tag in tags_dicts:
                assert len(tags_dicts[self._startup_tag]) == 1 # only appears once
                startup_time = self._in_records[tags_dicts[self._startup_tag][0]].cycles
                if Verbose:
                    print("start-up time:%f" % (counts_to_msec(startup_time)))
                return counts_to_msec(startup_time)
        return None
        # end if
    # end def get_starting_time
# end class Profiler


class Parser:
    def __init__(self):
        self._tmp_in_data = []
        self._prev_timestamp = 0

    def binary_stream_to_records(self, in_data) -> Tuple[tuple, ...]:
        self._tmp_in_data.extend(in_data)
        out_data = []
        while len(self._tmp_in_data) >= RECORD_SIZE:
            i = 0
            record_found = False
            for i in range(len(self._tmp_in_data) - MARKER_SIZE + 1):
                footer = int.from_bytes(self._tmp_in_data[i:i + MARKER_SIZE], byteorder='little', signed=False)
                if footer == FOOTER:
                    rec_limit = i + MARKER_SIZE
                    if rec_limit >= RECORD_SIZE:
                        header = int.from_bytes(self._tmp_in_data[rec_limit - RECORD_SIZE:rec_limit -
                                                    (RECORD_SIZE - MARKER_SIZE)], byteorder='little', signed=False)
                        if header == HEADER:
                            record_found = True
                            payload_start = rec_limit - (RECORD_SIZE - MARKER_SIZE)
                            record = tuple(self._tmp_in_data[payload_start:payload_start + PAYLOAD_SIZE])
                            out_data.append(record)
                            # remove from buffer all the data that has been parsed
                            self._tmp_in_data = self._tmp_in_data[rec_limit:len(self._tmp_in_data)]
                        break # get out of the loop that looks for a footer
                    # end if
                # end if
            # end for
            if not record_found:
                # remove from buffer all bytes that are guaranteed no to form part of a partial record
                # Worst case is that only one byte is missing to have a complete record, so we can
                # safely remove data until we leave RECORD_SIZE - 1 bytes in the buffer.
                # Note that 'i' iterates up to buffer_size - MARKER_SIZE
                self._tmp_in_data = self._tmp_in_data[(i - (RECORD_SIZE - MARKER_SIZE - 1)):len(self._tmp_in_data)]
            # end if
        # end while
        return tuple(out_data)
    # end def binary_stream_to_records

    @staticmethod
    def parse_records(raw_records) -> Tuple[ProfilerRecord, ...]:
        out_data = []
        for rec in raw_records:
            tag = int.from_bytes(rec[TAG_OFFSET:TAG_OFFSET + TAG_SIZE], byteorder='little', signed=False)
            cycles = int.from_bytes(rec[CYCLES_OFFSET:CYCLES_OFFSET + CYCLES_SIZE], byteorder='little', signed=False)
            seq_number = int.from_bytes(rec[SEQ_NUMBER_OFFSET:SEQ_NUMBER_OFFSET + SEQ_NUMBER_SIZE], byteorder='little', signed=False)
            out_data.append(ProfilerRecord(tag=tag, seq_number=seq_number, cycles=cycles))
        return tuple(out_data)
    # end def parse_records
# end class Parser


class ProfilerExecutor(threading.Thread):
    def __init__(self, jlink : JLink, parser : Parser, profiler : Profiler):
        super(ProfilerExecutor, self).__init__()
        self._jlink = jlink
        self._stop_event = threading.Event()
        self._parser = parser
        self._profiler = profiler
    # end def __init__

    def stop_capture(self):
        self._stop_event.set()
        self.join()
        self._profiler.process_data()
    # end def stop_capture

    def stop_requested(self):
        return self._stop_event.is_set()
    # end def stop_requested

    def run(self):
        """
        This method is a polling loop against the connected JLink unit. If
        the JLink is disconnected, it will exit. Additionally, if any exceptions
        are raised, they will be caught and re-raised after interrupting the
        main thread.

        Raises:
          Exception on error.

        """
        try:
            while self._jlink.connected() and not self.stop_requested():
                in_data = self._jlink.rtt_read(1, 1024)
                parsed_data = self._parser.parse_records(self._parser.binary_stream_to_records(in_data))
                if Verbose:
                    counters = tuple(record.seq_number for record in parsed_data)
                    print(counters)
                # end if
                if len(parsed_data) > 0:
                    self._profiler.add_data(parsed_data)
                # end if
                time.sleep(0.01)
            # self._profiler.process_data()
        except Exception:
            # this might require some rework as the exception must be catched by the main thread.
            warnings.warn("IO read thread exception, exiting...")
            _thread.interrupt_main()
            raise
        # end try
    # end def run

    def start_capture(self, rtt_block_address: Optional[int] = None):
        """
        Returns:
          Always returns ``0`` or a JLinkException.

        Raises:
          JLinkException on error.
        """
        if rtt_block_address is not None:
            self._jlink.exec_command(f'SetRTTAddr {hex(rtt_block_address)}')

        for i in range(201):
            try:
                if Verbose:
                    print("connected, starting RTT...")
                # end if
                self._jlink.rtt_start()
                num_up = self._jlink.rtt_get_num_up_buffers()
                num_down = self._jlink.rtt_get_num_down_buffers()
                if Verbose:
                    print("RTT started, %du p bufs, %d down bufs." % (num_up, num_down))
                # end if
                break
            except pylink.errors.JLinkRTTException as e:
                if Verbose:
                    print(e)
                # end if
                if i == 200:
                    raise e
                time.sleep(0.01)
            # end try
        # end for

        if self._jlink.halted():
            if Verbose:
                print("CPU halted")
            # end if
            while True:
                in_data = self._jlink.rtt_read(1, 1024)
                time.sleep(0.01)
                in_data = self._jlink.rtt_read(1, 1024)
                if len(in_data) == 0:
                    break
                if not self._jlink.connected() or self.stop_requested():
                    break
                # end if
            # end while


        super(ProfilerExecutor, self).start()
    # end def start_capture
# end class ProfilerExecutor
# ------------------------------------------------------------------------------
# END OF FILE
# ------------------------------------------------------------------------------
